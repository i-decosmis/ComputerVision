{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "from PIL import Image\n",
    "import os\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "from ptflops import get_model_complexity_info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this class we create a wrapper to the DeepLab one, allowing us to normalize the output\n",
    "# using sigmoid function\n",
    "class NormalizedDecoder(smp.DeepLabV3Plus):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = super().forward(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "# Dataset custom class, used to create the datasets that we will use during train loop and test loop\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform_image=None, transform_annotation=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform_image = transform_image\n",
    "        self.transform_annotation = transform_annotation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_dir[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        normalized_path = self.img_dir[0][idx]\n",
    "        image = Image.open(normalized_path).convert('RGB')\n",
    "        image = TF.to_tensor(image)\n",
    "        annotation_path = self.img_dir[1][idx]\n",
    "        annotation = Image.open(annotation_path)\n",
    "        annotation = TF.to_tensor(annotation)\n",
    "        if self.transform_image:\n",
    "            image = self.transform_image(image)\n",
    "        if self.transform_annotation:\n",
    "            annotation = self.transform_annotation(annotation)\n",
    "        return image, annotation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groundtruth_images(image_folder, folders):\n",
    "    path_annotations = []\n",
    "    for folder_name in folders:\n",
    "        folder_path = os.path.join(image_folder, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            tile_folder_path = os.path.join(folder_path, 'groundtruth')\n",
    "            for filename in os.listdir(tile_folder_path):\n",
    "                if filename.endswith('iMap.png'):\n",
    "                    image_path = os.path.join(tile_folder_path, filename)\n",
    "                    path_annotations.append(image_path)\n",
    "    return path_annotations\n",
    "\n",
    "# With this function we read all the paths for the images from the input path\n",
    "def get_normalized_paths(image_folder, folders):\n",
    "    train_paths = []\n",
    "    for folder_name in folders:\n",
    "        folder_path = os.path.join(image_folder, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            tile_folder_path = os.path.join(folder_path, 'tile', 'RGB')\n",
    "            for filename in os.listdir(tile_folder_path):\n",
    "                image_path = os.path.join(tile_folder_path, filename)\n",
    "                train_paths.append(image_path)\n",
    "    return train_paths\n",
    "\n",
    "\n",
    "# With this function we read all the paths for the images from the input path and we split them using the\n",
    "# % in input\n",
    "def get_normalized_paths_split(image_folder, folders, train_validation_split):\n",
    "    train_paths = []\n",
    "    validation_paths = []\n",
    "    for folder_name in folders:\n",
    "        folder_path = os.path.join(image_folder, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            tile_folder_path = os.path.join(folder_path, 'tile', 'RGB')\n",
    "            image_filenames = os.listdir(tile_folder_path)\n",
    "            num_train = int(len(image_filenames) * train_validation_split)\n",
    "            train_filenames = image_filenames[:num_train]\n",
    "            validation_filenames = image_filenames[num_train:]\n",
    "            for filename in train_filenames:\n",
    "                image_path = os.path.join(tile_folder_path, filename)\n",
    "                train_paths.append(image_path)\n",
    "            for filename in validation_filenames:\n",
    "                image_path = os.path.join(tile_folder_path, filename)\n",
    "                validation_paths.append(image_path)\n",
    "    return train_paths, validation_paths\n",
    "\n",
    "\n",
    "# With this function we read all the annotations from the input path\n",
    "def get_annotation_paths(annotation_folder: str, folders):\n",
    "    path_annotations = []\n",
    "    for folder_name in folders:\n",
    "        folder_path = os.path.join(annotation_folder, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            ndvi_folder = os.path.join(folder_path, 'tile', 'NDVI')\n",
    "            if os.path.exists(ndvi_folder):\n",
    "                for filename in os.listdir(ndvi_folder):\n",
    "                    if filename.endswith('.png'):\n",
    "                        image_path = os.path.join(ndvi_folder, filename)\n",
    "                        path_annotations.append(image_path)\n",
    "    return path_annotations\n",
    "\n",
    "# With this function we read all the paths for the annotations from the input path and we split them using the\n",
    "# % in input\n",
    "def get_annotation_paths_split(annotation_folder: str, folders, train_validation_split):\n",
    "    path_annotations = []\n",
    "    validation_path_annotations = []\n",
    "    for folder_name in folders:\n",
    "        folder_path = os.path.join(annotation_folder, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            ndvi_folder = os.path.join(folder_path, 'tile', 'NDVI')\n",
    "            if os.path.exists(ndvi_folder):\n",
    "                image_filenames = os.listdir(ndvi_folder)\n",
    "                num_train = int(len(image_filenames) * train_validation_split)\n",
    "                train_filenames = image_filenames[:num_train]\n",
    "                validation_filenames = image_filenames[num_train:]\n",
    "                for filename in train_filenames:\n",
    "                    if filename.endswith('.png'):\n",
    "                        image_path = os.path.join(ndvi_folder, filename)\n",
    "                        path_annotations.append(image_path)\n",
    "                for filename in validation_filenames:\n",
    "                    if filename.endswith('.png'):\n",
    "                        image_path = os.path.join(ndvi_folder, filename)\n",
    "                        validation_path_annotations.append(image_path)\n",
    "    return path_annotations, validation_path_annotations\n",
    "\n",
    "\n",
    "# We use this function to validate the model after an epoch during the train loop to check if the model has improved\n",
    "def validate_model(model, validation_loader):\n",
    "    model.eval()\n",
    "    mse_validation = torchmetrics.MeanSquaredError().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, annotations in validation_loader:\n",
    "            images = images.to(device)\n",
    "            annotations = annotations.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            mse_validation.update(outputs,annotations)\n",
    "\n",
    "    mse_validation = mse_validation.compute()\n",
    "    wandb.log({'mse_validation': mse_validation})\n",
    "    return mse_validation.item()\n",
    "\n",
    "\n",
    "# We use this function to create new tensor to test the error for each class, the value in input define\n",
    "# which class we are analyzing at the moment\n",
    "# The tensor image and the tensor annotations will have all their pixel that are not in the class we are analyzing\n",
    "# removed, and after they will be returned as output out the function\n",
    "def get_selected_pixels_tensor(image, annotations, imap, value):\n",
    "    # Crea una maschera booleana in base al valore desiderato\n",
    "    mask = imap == value\n",
    "    assert imap.shape == image.shape\n",
    "    new_image = image[mask]\n",
    "    new_annotations = annotations[mask]\n",
    "\n",
    "    return new_image, new_annotations\n",
    "\n",
    "\n",
    "# We use this function to return the correct grountruth tensor to calculate the error for each class\n",
    "def get_grountruth(groundtruth_input, idx):\n",
    "    groundtruth_return = None\n",
    "    j = 0\n",
    "    for other, groundtruth in groundtruth_input:\n",
    "        groundtruth_return = groundtruth\n",
    "        if j == idx:\n",
    "            break\n",
    "        j += 1\n",
    "    return groundtruth_return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining data trasformation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ivan\\AppData\\Local\\Temp\\ipykernel_2204\\1366740292.py:3: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  transforms.Resize((224, 224), interpolation=Image.NEAREST, antialias=False),\n",
      "C:\\Users\\Ivan\\AppData\\Local\\Temp\\ipykernel_2204\\1366740292.py:10: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  transforms.Resize((224, 224), interpolation=Image.NEAREST)\n",
      "C:\\Users\\Ivan\\AppData\\Local\\Temp\\ipykernel_2204\\1366740292.py:14: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  transforms.Resize((224, 224), interpolation=Image.NEAREST, antialias=False),\n",
      "C:\\Users\\Ivan\\AppData\\Local\\Temp\\ipykernel_2204\\1366740292.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  transforms.Resize((224, 224), interpolation=Image.NEAREST),\n"
     ]
    }
   ],
   "source": [
    "# We define transformations for the images that will be applied during the execution of the program\n",
    "data_transforms= transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=Image.NEAREST, antialias=False),\n",
    "    transforms.ConvertImageDtype(torch.float32),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[\n",
    "                         0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "annotation_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=Image.NEAREST)\n",
    "])\n",
    "\n",
    "data_transforms_augmentation = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=Image.NEAREST, antialias=False),\n",
    "    transforms.ConvertImageDtype(torch.float32),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[\n",
    "                         0.229, 0.224, 0.225]),\n",
    "    #Augmentation transform\n",
    "    transforms.RandomHorizontalFlip(p=0.5)\n",
    "])\n",
    "\n",
    "annotation_transforms_augmentation = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=Image.NEAREST),\n",
    "    # Augmentation transform\n",
    "    transforms.RandomHorizontalFlip(p=0.5)\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring wandb config and general parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mi-decosmis\u001b[0m (\u001b[33moctopus-canneller\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Ivan/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\Ivan\\Desktop\\cv_notebook\\wandb\\run-20230623_205238-uij85u3e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/octopus-canneller/uncategorized/runs/uij85u3e' target=\"_blank\">vital-wind-347</a></strong> to <a href='https://wandb.ai/octopus-canneller/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/octopus-canneller/uncategorized' target=\"_blank\">https://wandb.ai/octopus-canneller/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/octopus-canneller/uncategorized/runs/uij85u3e' target=\"_blank\">https://wandb.ai/octopus-canneller/uncategorized/runs/uij85u3e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Authenticate API key\n",
    "wandb.login(key=\"59882b3dbe33bfba4a2007aee502f5b9803bb409\")\n",
    "# Initialize Wandb\n",
    "wandb.init()\n",
    "\n",
    "# Setting batch size\n",
    "batch_size = 30\n",
    "\n",
    "# Define a configuration object\n",
    "config = wandb.config\n",
    "config.encoder_name = 'efficientnet-b2'\n",
    "config.encoder_weights = 'imagenet'\n",
    "config.classes = 1\n",
    "config.lr = 0.0002\n",
    "config.batch_size = batch_size\n",
    "config.num_epochs = 1000\n",
    "\n",
    "# Log the configuration to Wandb\n",
    "wandb.config.update(config)\n",
    "\n",
    "# Defining folders\n",
    "folder = 'RedEdge'\n",
    "train_folders = ['000', '001', '002', '004']\n",
    "test_folders = ['003']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating test set, validation set, training set and groundtruth set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth = get_groundtruth_images(folder, test_folders)\n",
    "\n",
    "groundtruth_train = get_groundtruth_images(folder, train_folders)\n",
    "\n",
    "# Getting normalized images from folders, splitted in 2 lists, one for train(80%) and one for validation(20%)\n",
    "normalized_train_paths = get_normalized_paths_split(folder, train_folders, 0.8)\n",
    "\n",
    "# Getting normalized images for test\n",
    "normalized_test_paths = get_normalized_paths(folder, test_folders)\n",
    "\n",
    "# Getting classes annotations from folders, splitted in 2 lists, one for train(80%) and one for validation(20%)\n",
    "train_annotations_paths = get_annotation_paths_split(folder, train_folders, 0.8)\n",
    "\n",
    "# Getting classes annotations from folders for test\n",
    "test_annotations_paths = get_annotation_paths(folder, test_folders)\n",
    "\n",
    "# Creating each different set with images and annotations\n",
    "train_matrix = np.array([normalized_train_paths[0], train_annotations_paths[0]])\n",
    "validation_matrix = np.array([normalized_train_paths[1], train_annotations_paths[1]])\n",
    "test_matrix = np.array([normalized_test_paths, test_annotations_paths])\n",
    "groundtruth_matrix = np.array([groundtruth, groundtruth])\n",
    "\n",
    "groundtruth_train_matrix = np.array([groundtruth_train, groundtruth_train])\n",
    "\n",
    "# Creating data loader for train, validation and test set\n",
    "train_data = CustomDataset(train_matrix, data_transforms_augmentation, annotation_transforms_augmentation)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "validation_data = CustomDataset(validation_matrix, data_transforms_augmentation, annotation_transforms_augmentation)\n",
    "validation_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_data = CustomDataset(test_matrix, data_transforms,annotation_transforms)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "groundtruth_data = CustomDataset(groundtruth_matrix, annotation_transforms, annotation_transforms)\n",
    "groundtruth_loader = DataLoader(groundtruth_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "groundtruth_train_data = CustomDataset(groundtruth_train_matrix, annotation_transforms, annotation_transforms)\n",
    "groundtruth_train_loader = DataLoader(groundtruth_train_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating torch device, model, and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the device to gpu if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Creating the model\n",
    "model = NormalizedDecoder(\n",
    "    encoder_name='efficientnet-b2',\n",
    "    encoder_weights='imagenet',\n",
    "    classes=1\n",
    ").to(device)\n",
    "\n",
    "# Defining loss function for the train loop to check if model is improving each epoch\n",
    "mse_train = torchmetrics.MeanSquaredError().to(device)\n",
    "mae_train = torchmetrics.MeanAbsoluteError().to(device)\n",
    "\n",
    "# Defining function to calculate mean error after the inference loop\n",
    "mse_test = torchmetrics.MeanSquaredError().to(device)\n",
    "mae_test = torchmetrics.MeanAbsoluteError().to(device)\n",
    "\n",
    "mse_test_one = torchmetrics.MeanSquaredError().to(device)\n",
    "mse_test_two = torchmetrics.MeanSquaredError().to(device)\n",
    "mse_test_three = torchmetrics.MeanSquaredError().to(device)\n",
    "\n",
    "mae_test_one = torchmetrics.MeanAbsoluteError().to(device)\n",
    "mae_test_two = torchmetrics.MeanAbsoluteError().to(device)\n",
    "mae_test_three = torchmetrics.MeanAbsoluteError().to(device)\n",
    "\n",
    "mse_class_average = torchmetrics.MeanSquaredError().to(device)\n",
    "mae_class_average = torchmetrics.MeanAbsoluteError().to(device)\n",
    "\n",
    "mse_train_one = torchmetrics.MeanSquaredError().to(device)\n",
    "mse_train_two = torchmetrics.MeanSquaredError().to(device)\n",
    "mse_train_three = torchmetrics.MeanSquaredError().to(device)\n",
    "\n",
    "mae_train_one = torchmetrics.MeanAbsoluteError().to(device)\n",
    "mae_train_two = torchmetrics.MeanAbsoluteError().to(device)\n",
    "mae_train_three = torchmetrics.MeanAbsoluteError().to(device)\n",
    "\n",
    "\n",
    "# Wrappo il modello e l'optimizer con wandb\n",
    "wandb.watch(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing set elements and total number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in the train set: 362\n",
      "Number of elements in the validation set: 93\n",
      "Number of elements in the test set: 102\n",
      "Total number of parameters: 8642739\n"
     ]
    }
   ],
   "source": [
    "# Showing data in console\n",
    "print(\"Number of elements in the train set:\", len(train_data))\n",
    "print(\"Number of elements in the validation set:\", len(validation_data))\n",
    "print(\"Number of elements in the test set:\", len(test_data))\n",
    "\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total number of parameters: {total_params}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining hyperparameters and training loop for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1000 | Mse loss: 0.1247\n",
      "entro: inf\n",
      "Epoch: 2/1000 | Mse loss: 0.0639\n",
      "entro: 0.10802991688251495\n",
      "Epoch: 3/1000 | Mse loss: 0.0163\n",
      "entro: 0.019754493609070778\n",
      "Epoch: 4/1000 | Mse loss: 0.0146\n",
      "entro: 0.017978545278310776\n",
      "Epoch: 5/1000 | Mse loss: 0.0155\n",
      "entro: 0.016914425417780876\n",
      "Epoch: 6/1000 | Mse loss: 0.0155\n",
      "entro: 0.01580314338207245\n",
      "Epoch: 7/1000 | Mse loss: 0.0127\n",
      "entro: 0.013095824979245663\n",
      "Epoch: 8/1000 | Mse loss: 0.0124\n",
      "entro: 0.012529371306300163\n",
      "Epoch: 9/1000 | Mse loss: 0.0120\n",
      "Epoch: 10/1000 | Mse loss: 0.0121\n",
      "entro: 0.01208651252090931\n",
      "Epoch: 11/1000 | Mse loss: 0.0118\n",
      "Epoch: 12/1000 | Mse loss: 0.0121\n",
      "entro: 0.011679112911224365\n",
      "Epoch: 13/1000 | Mse loss: 0.0117\n",
      "Epoch: 14/1000 | Mse loss: 0.0113\n",
      "entro: 0.01147305965423584\n",
      "Epoch: 15/1000 | Mse loss: 0.0110\n",
      "Epoch: 16/1000 | Mse loss: 0.0112\n",
      "entro: 0.011099523864686489\n",
      "Epoch: 17/1000 | Mse loss: 0.0111\n",
      "entro: 0.01088991854339838\n",
      "Epoch: 18/1000 | Mse loss: 0.0109\n",
      "Epoch: 19/1000 | Mse loss: 0.0108\n",
      "Epoch: 20/1000 | Mse loss: 0.0101\n",
      "entro: 0.010359877720475197\n",
      "Epoch: 21/1000 | Mse loss: 0.0105\n",
      "Epoch: 22/1000 | Mse loss: 0.0101\n",
      "entro: 0.01023906096816063\n",
      "Epoch: 23/1000 | Mse loss: 0.0104\n",
      "Epoch: 24/1000 | Mse loss: 0.0102\n",
      "Epoch: 25/1000 | Mse loss: 0.0104\n",
      "Epoch: 26/1000 | Mse loss: 0.0107\n",
      "Epoch: 27/1000 | Mse loss: 0.0105\n",
      "Epoch: 28/1000 | Mse loss: 0.0110\n",
      "entro: 0.010078907012939453\n",
      "Epoch: 29/1000 | Mse loss: 0.0103\n",
      "Epoch: 30/1000 | Mse loss: 0.0101\n",
      "Epoch: 31/1000 | Mse loss: 0.0106\n",
      "Epoch: 32/1000 | Mse loss: 0.0100\n",
      "entro: 0.00991279911249876\n",
      "Epoch: 33/1000 | Mse loss: 0.0096\n",
      "Epoch: 34/1000 | Mse loss: 0.0099\n",
      "Epoch: 35/1000 | Mse loss: 0.0097\n",
      "Epoch: 36/1000 | Mse loss: 0.0099\n",
      "entro: 0.009599169716238976\n",
      "Epoch: 37/1000 | Mse loss: 0.0094\n",
      "Epoch: 38/1000 | Mse loss: 0.0094\n",
      "Epoch: 39/1000 | Mse loss: 0.0094\n",
      "Epoch: 40/1000 | Mse loss: 0.0092\n",
      "Epoch: 41/1000 | Mse loss: 0.0094\n",
      "entro: 0.00937061570584774\n",
      "Epoch: 42/1000 | Mse loss: 0.0092\n",
      "Epoch: 43/1000 | Mse loss: 0.0092\n",
      "Epoch: 44/1000 | Mse loss: 0.0093\n",
      "Epoch: 45/1000 | Mse loss: 0.0091\n",
      "Epoch: 46/1000 | Mse loss: 0.0097\n",
      "Epoch: 47/1000 | Mse loss: 0.0094\n",
      "Epoch: 48/1000 | Mse loss: 0.0093\n",
      "Epoch: 49/1000 | Mse loss: 0.0089\n",
      "Epoch: 50/1000 | Mse loss: 0.0093\n",
      "Epoch: 51/1000 | Mse loss: 0.0093\n",
      "Epoch: 52/1000 | Mse loss: 0.0090\n",
      "Epoch: 53/1000 | Mse loss: 0.0092\n",
      "Epoch: 54/1000 | Mse loss: 0.0094\n",
      "Epoch: 55/1000 | Mse loss: 0.0094\n",
      "entro: 0.009126433171331882\n",
      "Epoch: 56/1000 | Mse loss: 0.0094\n",
      "Epoch: 57/1000 | Mse loss: 0.0094\n",
      "Epoch: 58/1000 | Mse loss: 0.0098\n",
      "Epoch: 59/1000 | Mse loss: 0.0097\n",
      "Epoch: 60/1000 | Mse loss: 0.0095\n",
      "Epoch: 61/1000 | Mse loss: 0.0093\n",
      "Epoch: 62/1000 | Mse loss: 0.0095\n",
      "Epoch: 63/1000 | Mse loss: 0.0093\n",
      "Epoch: 64/1000 | Mse loss: 0.0093\n",
      "Epoch: 65/1000 | Mse loss: 0.0092\n",
      "Epoch: 66/1000 | Mse loss: 0.0094\n",
      "Epoch: 67/1000 | Mse loss: 0.0094\n",
      "Epoch: 68/1000 | Mse loss: 0.0093\n",
      "Epoch: 69/1000 | Mse loss: 0.0090\n",
      "Epoch: 70/1000 | Mse loss: 0.0089\n",
      "Epoch: 71/1000 | Mse loss: 0.0089\n",
      "Epoch: 72/1000 | Mse loss: 0.0090\n",
      "Epoch: 73/1000 | Mse loss: 0.0089\n",
      "Epoch: 74/1000 | Mse loss: 0.0091\n",
      "Epoch: 75/1000 | Mse loss: 0.0089\n",
      "Epoch: 76/1000 | Mse loss: 0.0091\n",
      "Epoch: 77/1000 | Mse loss: 0.0089\n",
      "Epoch: 78/1000 | Mse loss: 0.0089\n",
      "Epoch: 79/1000 | Mse loss: 0.0089\n",
      "Epoch: 80/1000 | Mse loss: 0.0089\n",
      "Epoch: 81/1000 | Mse loss: 0.0089\n",
      "Epoch: 82/1000 | Mse loss: 0.0088\n",
      "entro: 0.008925221860408783\n",
      "Epoch: 83/1000 | Mse loss: 0.0089\n",
      "Epoch: 84/1000 | Mse loss: 0.0089\n",
      "Epoch: 85/1000 | Mse loss: 0.0087\n",
      "Epoch: 86/1000 | Mse loss: 0.0090\n",
      "Epoch: 87/1000 | Mse loss: 0.0089\n",
      "Epoch: 88/1000 | Mse loss: 0.0090\n",
      "Epoch: 89/1000 | Mse loss: 0.0088\n",
      "Epoch: 90/1000 | Mse loss: 0.0090\n",
      "Epoch: 91/1000 | Mse loss: 0.0092\n",
      "Epoch: 92/1000 | Mse loss: 0.0092\n",
      "Epoch: 93/1000 | Mse loss: 0.0090\n",
      "Epoch: 94/1000 | Mse loss: 0.0093\n",
      "Epoch: 95/1000 | Mse loss: 0.0091\n",
      "Epoch: 96/1000 | Mse loss: 0.0092\n",
      "Epoch: 97/1000 | Mse loss: 0.0090\n",
      "Epoch: 98/1000 | Mse loss: 0.0099\n",
      "Epoch: 99/1000 | Mse loss: 0.0094\n",
      "Epoch: 100/1000 | Mse loss: 0.0093\n",
      "Epoch: 101/1000 | Mse loss: 0.0092\n",
      "Epoch: 102/1000 | Mse loss: 0.0095\n",
      "Epoch: 103/1000 | Mse loss: 0.0093\n",
      "Epoch: 104/1000 | Mse loss: 0.0091\n",
      "Epoch: 105/1000 | Mse loss: 0.0091\n",
      "Epoch: 106/1000 | Mse loss: 0.0090\n",
      "entro: 0.00881913211196661\n",
      "Epoch: 107/1000 | Mse loss: 0.0087\n",
      "Epoch: 108/1000 | Mse loss: 0.0091\n",
      "Epoch: 109/1000 | Mse loss: 0.0091\n",
      "Epoch: 110/1000 | Mse loss: 0.0094\n",
      "Epoch: 111/1000 | Mse loss: 0.0087\n",
      "Epoch: 112/1000 | Mse loss: 0.0089\n",
      "entro: 0.008688045665621758\n",
      "Epoch: 113/1000 | Mse loss: 0.0090\n",
      "Epoch: 114/1000 | Mse loss: 0.0090\n",
      "Epoch: 115/1000 | Mse loss: 0.0085\n",
      "Epoch: 116/1000 | Mse loss: 0.0088\n",
      "Epoch: 117/1000 | Mse loss: 0.0091\n",
      "Epoch: 118/1000 | Mse loss: 0.0088\n",
      "Epoch: 119/1000 | Mse loss: 0.0085\n",
      "Epoch: 120/1000 | Mse loss: 0.0087\n",
      "Epoch: 121/1000 | Mse loss: 0.0086\n",
      "Epoch: 122/1000 | Mse loss: 0.0086\n",
      "Epoch: 123/1000 | Mse loss: 0.0087\n",
      "Epoch: 124/1000 | Mse loss: 0.0088\n",
      "Epoch: 125/1000 | Mse loss: 0.0087\n",
      "Epoch: 126/1000 | Mse loss: 0.0087\n",
      "Epoch: 127/1000 | Mse loss: 0.0086\n",
      "Epoch: 128/1000 | Mse loss: 0.0087\n",
      "Epoch: 129/1000 | Mse loss: 0.0087\n",
      "entro: 0.00857615564018488\n",
      "Epoch: 130/1000 | Mse loss: 0.0085\n",
      "Epoch: 131/1000 | Mse loss: 0.0089\n",
      "Epoch: 132/1000 | Mse loss: 0.0090\n",
      "Epoch: 133/1000 | Mse loss: 0.0089\n",
      "Epoch: 134/1000 | Mse loss: 0.0089\n",
      "Epoch: 135/1000 | Mse loss: 0.0086\n",
      "Epoch: 136/1000 | Mse loss: 0.0088\n",
      "Epoch: 137/1000 | Mse loss: 0.0088\n",
      "Epoch: 138/1000 | Mse loss: 0.0090\n",
      "Epoch: 139/1000 | Mse loss: 0.0087\n",
      "Epoch: 140/1000 | Mse loss: 0.0087\n",
      "Epoch: 141/1000 | Mse loss: 0.0089\n",
      "Epoch: 142/1000 | Mse loss: 0.0086\n",
      "Epoch: 143/1000 | Mse loss: 0.0087\n",
      "Epoch: 144/1000 | Mse loss: 0.0086\n",
      "Epoch: 145/1000 | Mse loss: 0.0086\n",
      "Epoch: 146/1000 | Mse loss: 0.0087\n",
      "Epoch: 147/1000 | Mse loss: 0.0088\n",
      "Epoch: 148/1000 | Mse loss: 0.0088\n",
      "Epoch: 149/1000 | Mse loss: 0.0086\n",
      "Epoch: 150/1000 | Mse loss: 0.0087\n",
      "Epoch: 151/1000 | Mse loss: 0.0085\n",
      "Epoch: 152/1000 | Mse loss: 0.0086\n",
      "Epoch: 153/1000 | Mse loss: 0.0086\n",
      "Epoch: 154/1000 | Mse loss: 0.0087\n",
      "Epoch: 155/1000 | Mse loss: 0.0088\n",
      "Epoch: 156/1000 | Mse loss: 0.0085\n",
      "Epoch: 157/1000 | Mse loss: 0.0087\n",
      "Epoch: 158/1000 | Mse loss: 0.0083\n",
      "Epoch: 159/1000 | Mse loss: 0.0088\n",
      "Epoch: 160/1000 | Mse loss: 0.0085\n",
      "Epoch: 161/1000 | Mse loss: 0.0087\n",
      "entro: 0.00843283161520958\n",
      "Epoch: 162/1000 | Mse loss: 0.0088\n",
      "Epoch: 163/1000 | Mse loss: 0.0086\n",
      "Epoch: 164/1000 | Mse loss: 0.0086\n",
      "Epoch: 165/1000 | Mse loss: 0.0084\n",
      "Epoch: 166/1000 | Mse loss: 0.0086\n",
      "Epoch: 167/1000 | Mse loss: 0.0084\n",
      "Epoch: 168/1000 | Mse loss: 0.0086\n",
      "Epoch: 169/1000 | Mse loss: 0.0087\n",
      "Epoch: 170/1000 | Mse loss: 0.0084\n",
      "Epoch: 171/1000 | Mse loss: 0.0084\n",
      "Epoch: 172/1000 | Mse loss: 0.0084\n",
      "Epoch: 173/1000 | Mse loss: 0.0086\n",
      "Epoch: 174/1000 | Mse loss: 0.0083\n",
      "Epoch: 175/1000 | Mse loss: 0.0085\n",
      "Epoch: 176/1000 | Mse loss: 0.0088\n",
      "Epoch: 177/1000 | Mse loss: 0.0085\n",
      "Epoch: 178/1000 | Mse loss: 0.0082\n",
      "Epoch: 179/1000 | Mse loss: 0.0089\n",
      "Epoch: 180/1000 | Mse loss: 0.0088\n",
      "Epoch: 181/1000 | Mse loss: 0.0085\n",
      "Epoch: 182/1000 | Mse loss: 0.0087\n",
      "Epoch: 183/1000 | Mse loss: 0.0086\n",
      "Epoch: 184/1000 | Mse loss: 0.0088\n",
      "Epoch: 185/1000 | Mse loss: 0.0088\n",
      "Epoch: 186/1000 | Mse loss: 0.0085\n",
      "Epoch: 187/1000 | Mse loss: 0.0083\n",
      "Epoch: 188/1000 | Mse loss: 0.0085\n",
      "Epoch: 189/1000 | Mse loss: 0.0082\n",
      "Epoch: 190/1000 | Mse loss: 0.0083\n",
      "Epoch: 191/1000 | Mse loss: 0.0086\n",
      "Epoch: 192/1000 | Mse loss: 0.0086\n",
      "Epoch: 193/1000 | Mse loss: 0.0084\n",
      "Epoch: 194/1000 | Mse loss: 0.0086\n",
      "Epoch: 195/1000 | Mse loss: 0.0088\n",
      "Epoch: 196/1000 | Mse loss: 0.0086\n",
      "Epoch: 197/1000 | Mse loss: 0.0084\n",
      "Epoch: 198/1000 | Mse loss: 0.0085\n",
      "Epoch: 199/1000 | Mse loss: 0.0086\n",
      "Epoch: 200/1000 | Mse loss: 0.0087\n",
      "Epoch: 201/1000 | Mse loss: 0.0083\n",
      "Stopping at epoch: 201\n",
      "Using the model saved from epoch: 161\n"
     ]
    }
   ],
   "source": [
    "# Variables for training loop\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)\n",
    "num_epochs = 1000\n",
    "best_loss = np.inf\n",
    "patience = 40\n",
    "delta_threshold = 0.0001\n",
    "counter = 0\n",
    "saved_epoch = 0\n",
    "# Training the model\n",
    "for epoch in range(num_epochs):\n",
    "    cicle = enumerate(train_loader)\n",
    "    j = 0\n",
    "    for i, (images, annotations) in cicle:\n",
    "        images = images.to(device)\n",
    "        annotations = annotations.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, annotations)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        mse_train.update(outputs, annotations)\n",
    "        mae_train.update(outputs, annotations)\n",
    "        groundtruth_test = get_grountruth(groundtruth_loader, j)\n",
    "        for img, annotation, img_groundtruth in zip(outputs, annotations, groundtruth_test):\n",
    "            new_img, new_groundtruth = get_selected_pixels_tensor(img, annotation, img_groundtruth, 0)\n",
    "            mse_train_one.update(new_img, new_groundtruth)\n",
    "            mae_train_one.update(new_img, new_groundtruth)\n",
    "        for img, annotation, img_groundtruth in zip(outputs, annotations, groundtruth_test):\n",
    "            new_img, new_groundtruth = get_selected_pixels_tensor(img, annotation, img_groundtruth, 2)\n",
    "            mse_train_two.update(new_img, new_groundtruth)\n",
    "            mae_train_two.update(new_img, new_groundtruth)\n",
    "        for img, annotation, img_groundtruth in zip(outputs, annotations, groundtruth_test):\n",
    "            new_img, new_groundtruth = get_selected_pixels_tensor(img, annotation, img_groundtruth, 10000)\n",
    "            mse_train_three.update(new_img, new_groundtruth)\n",
    "            mae_train_three.update(new_img, new_groundtruth)\n",
    "        j += 1\n",
    "    # Verifica se la loss corrente è migliore della migliore loss finora\n",
    "    mse_train = mse_train.compute()\n",
    "    mae_train = mae_train.compute()\n",
    "    mse_train_one = mse_train_one.compute()\n",
    "    mse_train_two = mse_train_two.compute()\n",
    "    mse_train_three = mse_train_three.compute()\n",
    "\n",
    "    mae_train_one = mae_train_one.compute()\n",
    "    mae_train_two = mae_train_two.compute()\n",
    "    mae_train_three = mae_train_three.compute()\n",
    "    print(f\"Epoch: {epoch + 1}/{num_epochs} | Mse loss: {mse_train.item():.4f}\")\n",
    "    wandb.log({'mse_training': mse_train, 'mae_training': mae_train, 'mse_train_class_one': mse_train_one,\n",
    "               'mse_train_class_two': mse_train_two, 'mse_train_class_thress': mse_train_three,\n",
    "               'mae_train_class_one': mae_train_one, 'mae_train_class_two': mae_train_two,\n",
    "               'mae_train_class_three': mae_train_three})\n",
    "    mse_train = torchmetrics.MeanSquaredError().to(device)\n",
    "    mae_train = torchmetrics.MeanAbsoluteError().to(device)\n",
    "    mse_train_one = torchmetrics.MeanSquaredError().to(device)\n",
    "    mse_train_two = torchmetrics.MeanSquaredError().to(device)\n",
    "    mse_train_three = torchmetrics.MeanSquaredError().to(device)\n",
    "\n",
    "    mae_train_one = torchmetrics.MeanAbsoluteError().to(device)\n",
    "    mae_train_two = torchmetrics.MeanAbsoluteError().to(device)\n",
    "    mae_train_three = torchmetrics.MeanAbsoluteError().to(device)\n",
    "    mse_validation = validate_model(model, validation_loader)\n",
    "    if best_loss - mse_validation > delta_threshold:\n",
    "        print(\"entro:\", best_loss)\n",
    "        saved_epoch = epoch + 1\n",
    "        best_loss = mse_validation\n",
    "        counter = 0  # Reimposta il contatore\n",
    "        torch.save(model.state_dict(), 'model_backup')\n",
    "    else:\n",
    "        counter += 1\n",
    "    if counter >= patience:\n",
    "        print(f\"Stopping at epoch: {epoch + 1}\")\n",
    "        print(\"Using the model saved from epoch:\", saved_epoch)\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the complexity of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational complexity:       436.09 MMac\n",
      "Number of parameters:           8.64 M  \n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.device(0):\n",
    "    macs, params = get_model_complexity_info(model, (3, 224, 224), as_strings=True,\n",
    "                                           print_per_layer_stat=False, verbose=False)\n",
    "    print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "    print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference loop with testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the last better model and setting it in evaluation mode\n",
    "model.load_state_dict(torch.load('model_backup'))\n",
    "model.eval()\n",
    "\n",
    "# Starting inference with test data\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    counter = 1\n",
    "    for images, annotations in test_loader:\n",
    "        images = images.to(device)\n",
    "        annotations = annotations.to(device)\n",
    "        # Passaggio Forward\n",
    "        outputs = model.predict(images)\n",
    "        # Updating error\n",
    "        mse_test.update(outputs,annotations)\n",
    "        mae_test.update(outputs,annotations)\n",
    "        groundtruth_test = get_grountruth(groundtruth_loader, i)\n",
    "        for img, annotation, img_groundtruth in zip(outputs, annotations, groundtruth_test):\n",
    "            new_img, new_groundtruth = get_selected_pixels_tensor(img, annotation, img_groundtruth, 0)\n",
    "            mse_test_one.update(new_img, new_groundtruth)\n",
    "            mae_test_one.update(new_img, new_groundtruth)\n",
    "        for img, annotation, img_groundtruth in zip(outputs, annotations, groundtruth_test):\n",
    "            new_img, new_groundtruth = get_selected_pixels_tensor(img, annotation, img_groundtruth, 2)\n",
    "            mse_test_two.update(new_img, new_groundtruth)\n",
    "            mae_test_two.update(new_img, new_groundtruth)\n",
    "        for img, annotation, img_groundtruth in zip(outputs, annotations, groundtruth_test):\n",
    "            new_img, new_groundtruth = get_selected_pixels_tensor(img, annotation, img_groundtruth, 10000)\n",
    "            mse_test_three.update(new_img, new_groundtruth)\n",
    "            mae_test_three.update(new_img, new_groundtruth)\n",
    "        i += 1\n",
    "\n",
    "        # Creating wandb table for logs images for easy comparison\n",
    "        table = wandb.Table(columns=['Type', 'Image'])\n",
    "        input_images = wandb.Image(\n",
    "            images\n",
    "        )\n",
    "        annotations_images = wandb.Image(\n",
    "            annotations\n",
    "        )\n",
    "        output_images = wandb.Image(\n",
    "            outputs\n",
    "        )\n",
    "        table.add_data(\"Input\", input_images)\n",
    "        table.add_data(\"Ground truth\", annotations_images)\n",
    "        table.add_data(\"Output\", output_images)\n",
    "        wandb.log({'Table_{}'.format(counter): table})\n",
    "        counter += 1\n",
    "    # Computing total error\n",
    "    mse_test = mse_test.compute()\n",
    "    mae_test = mae_test.compute()\n",
    "    mse_test_one = mse_test_one.compute()\n",
    "    mse_test_two = mse_test_two.compute()\n",
    "    mse_test_three = mse_test_three.compute()\n",
    "    mae_test_one = mae_test_one.compute()\n",
    "    mae_test_two = mae_test_two.compute()\n",
    "    mae_test_three = mae_test_three.compute()\n",
    "    mae_test_average = (mae_test_one.item() + mae_test_two.item() + mae_test_three.item()) / 3\n",
    "    mse_test_average = (mse_test_one.item() + mse_test_two.item() + mse_test_three.item()) / 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging last information on wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logging average error for classes after inference\n",
    "wandb.summary[\"MSE microaveraged\"] = mse_test\n",
    "wandb.summary[\"MAE microaveraged\"] = mae_test\n",
    "wandb.summary[\"mae_class_one\"] = mae_test_one\n",
    "wandb.summary[\"mae_class_two\"] = mae_test_two\n",
    "wandb.summary[\"mae_class_three\"] = mae_test_three\n",
    "wandb.summary[\"mse_class_one\"] = mse_test_one\n",
    "wandb.summary[\"mse_class_two\"] = mse_test_two\n",
    "wandb.summary[\"mse_class_three\"] = mse_test_three\n",
    "wandb.summary[\"MAE macroaveraged\"] = mae_test_average\n",
    "wandb.summary[\"MSE macroaveraged\"] = mse_test_average\n",
    "\n",
    "\n",
    "# Saving logs locally\n",
    "wandb.save('model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
